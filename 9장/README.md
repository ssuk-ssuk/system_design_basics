# 9장: 웹 크롤러 설계

## 1. 웹 크롤러의 정의 및 용도

웹 크롤러는 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 기술로, 하이퍼링크를 따라가며 정보를 수집합니다.

* **검색 엔진 인덱싱**: Googlebot과 같이 웹 페이지를 모아 로컬 인덱스를 생성.
* **웹 아카이빙**: 나중에 사용할 목적으로 웹 정보를 장기 보관.
* **웹 마이닝/모니터링**: 유용한 지식 도출 및 저작권 침해 사례 모니터링.

## 2. 개략적 규모 추정 (검색 엔진 인덱싱 용도)

* **수집 목표**: 매달 10억 개의 웹 페이지.
* **QPS (Queries Per Second)**: 평균 약 400페이지/초, 최대 800페이지/초.
* **저장 용량**: 페이지당 평균 500k 가정 시, 1개월에 500TB, 5년 보관 시 약 30PB 필요.

## 3. 웹 크롤러 워크플로우 (작업 흐름)

1. **시작 URL 집합**: 크롤링의 출발점이며 범위를 결정.
2. **미수집 URL 저장소 (URL Frontier)**: 방문할 URL을 관리하는 'To-do List'.
3. **HTML 다운로더**: DNS를 통해 IP를 알아내고 웹 페이지를 다운로드.
4. **콘텐츠 파서**: 다운로드된 데이터를 검증하고 유의미한 정보로 정제.
5. **중복 콘텐츠 판별**: 이미 저장된 콘텐츠인지 확인하여 중복 처리 방지.
6. **URL 추출기**: HTML 페이지 내 `<href>` 태그에서 새로운 링크 추출.
7. **URL 필터**: 특정 확장자나 제외 목록에 있는 URL 배제.
8. **이미 방문한 URL 판별**: 중복 방문을 막기 위해 저장소 확인.

## 4. 핵심 기술 및 상세 구현

### 4.1 중복 콘텐츠 판별 (SimHash)

일반 해시(MD5 등)는 1비트만 바뀌어도 결과가 달라지므로 웹 페이지 중복 판별에는 적합하지 않습니다.

* **유사도 해시 (SimHash)**: 비슷한 내용은 비슷한 해시값을 가지도록 설계된 알고리즘.
* **해밍 거리 (Hamming Distance)**: 두 비트열 사이의 서로 다른 비트 개수를 계산하여 유사도 판단.
* **정문화 (Normalization)**: 태그, 광고, 공백 등을 제거하는 전처리 과정 필수.

### 4.2 미수집 URL 저장소 (Politeness & Priority)

크롤러는 예의(Politeness)를 갖추고 우선순위를 고려해야 합니다.

* **전면 큐 (Front Queues)**: URL의 중요도에 따라 우선순위 결정.
* **후면 큐 (Back Queues)**: 같은 호스트(도메인)당 하나의 큐를 할당하여 특정 서버 과부하 방지.
* **큐 라우터 & 선택기**: 매핑 테이블을 참조해 특정 호스트를 특정 큐로 보내고 순차적으로 처리.


## 5. 성능 최적화 및 안정성

5.1 성능 최적화 기법 

* **분산 크롤링**: 크롤링 작업을 여러 서버 및 스레드에 분산.
* **DNS 결과 캐싱**: 동기적인 DNS 조회를 병목 지점으로 보아 캐시로 관리.
* **지역성**: 크롤링 서버를 대상 서버와 지리적으로 가깝게 배치.
* **짧은 타임아웃**: 응답 없는 서버에 대해 무한정 기다리지 않음.



5.2 안정성 및 확장성 

* **안정 해시**: 다운로더 서버 간 부하 분산.
* **Robots.txt 확인**: 웹사이트의 수집 허용 규칙 준수.
* **거미 덫 (Spider Trap) 회피**: 무한 루프 사이트를 감지하고 URL 길이 제한 설정.
* **확장성 모듈**: PNG 다운로더나 웹 모니터 같은 새로운 콘텐츠 유형 지원.



## 6. 마무리 및 추가 논의

* **서버 측 렌더링 (SSR)**: 동적으로 생성되는 링크를 찾기 위해 Selenium 등을 활용한 페이지 렌더링 필요.
* **데이터베이스 샤딩**: 저장소의 가용성과 안정성 향상.
